# Baby-Face-Generator
ComfyUI Workflow(Baby-Face-Generator)

ğŸ‘¶ Baby Face Generator (Parent-to-Child Image Generation)

This project demonstrates how to generate a synthetic baby portrait from two parent images using Stable Diffusion + IPAdapter + CLIP Vision inside ComfyUI.
The workflow encodes parent facial features via IPAdapter, blends them, and conditions Stable Diffusion with prompts to produce realistic baby-like outputs.

âš ï¸ Disclaimer: This is a synthetic AI workflow for demonstration purposes only.
It does not predict actual genetics or real child appearance.

ğŸš€ Features

ğŸ§© ComfyUI Workflow â€“ modular, drag-and-drop nodes for reproducibility.

ğŸ”— IPAdapter + CLIP Vision â€“ encode and blend parent facial features.

ğŸ¨ Stable Diffusion â€“ conditioned sampling for realistic baby portraits.

âš™ï¸ Tunable Parameters â€“ control weights, CFG scale, steps, and prompts.

ğŸ“¦ Easy Export â€“ workflow available as JSON + PNG (drag-and-drop).

ğŸ“‚ Project Structure
Baby-Face-Generator/
â”œâ”€â”€ workflow.json        # Importable ComfyUI workflow
â”œâ”€â”€ workflow.png         # Visual workflow (also contains embedded JSON)
â”œâ”€â”€ samples/             # Example baby outputs
â”‚   â”œâ”€â”€ parentA.png
â”‚   â”œâ”€â”€ parentB.png
â”‚   â”œâ”€â”€ sample1.png
â”‚   â”œâ”€â”€ sample2.png
â”œâ”€â”€ README.md            # Project documentation

ğŸ› ï¸ Requirements

ComfyUI
 (latest build)

Extension: ComfyUI_IPAdapter_plus

Python 3.10+ with:

pip install torch torchvision opencv-python

Models Required

Stable Diffusion 1.5 checkpoint
Place in:

ComfyUI/models/checkpoints/


IPAdapter weights (ip-adapter-full-face_sd15.bin)
Download from IPAdapter Hugging Face
 and place in:

ComfyUI/models/ipadapter/


CLIP Vision backbone (clip-vit-h-14)
Download from laion/CLIP-ViT-H-14
 and place in:

ComfyUI/models/clip_vision/clip-vit-h-14/

ğŸ§© Workflow Explanation
1. Parent Image Processing

Load Image + Resize Image â†’ preprocess parent photos (512Ã—512).

Images are passed into IPAdapter Advanced with the SD checkpoint + adapter model.

2. Model Conditioning

Each IPAdapter Advanced node outputs a model conditioned on one parent face.

Model Merge blends both models (default: 50/50 ratio).

3. Text Prompts

Positive Prompt:

photorealistic baby, 6 months old, soft smooth skin, rounded cheeks, DSLR portrait, natural lighting, high detail


Negative Prompt:

adult face, old person, cartoon, doll, anime, cgi, deformed, blurry, bad anatomy, watermark, text

4. Sampling

KSampler generates the output latent with:

Sampler: Euler a

Steps: 25â€“30

CFG Scale: 7.0

Seed: random or fixed

5. Output

VAE Decode converts latent â†’ RGB baby portrait.

Optional:

GFPGAN (face restoration)

Real-ESRGAN (upscaling)

ğŸ›ï¸ Tuning Tips

Parent Weights:

Equal (0.5/0.5) â†’ balanced features

Biased (0.6/0.4) â†’ favor one parent

CFG Scale:

6.5â€“8.0 for realism & prompt adherence

Sampler Steps:

25 for speed, 35â€“40 for higher detail

Face Restore:

Use GFPGAN strength ~0.3â€“0.5 (too high makes faces generic)

ğŸ–¼ï¸ Example Results
Parent A	Parent B	Generated Baby

	
	
ğŸ“œ Notes

This workflow is designed for AI demo purposes.

It demonstrates skills in workflow engineering, model integration, and prompt tuning using ComfyUI.

It does not provide real genetic predictions.

ğŸ‘¨â€ğŸ’» Author

Dhruvit Navadiya â€“ AI/ML Engineer
ğŸ“Œ LinkedIn
 | GitHub

